{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load your data from a CSV file into a pandas DataFrame\n",
    "encoded_df = pd.read_csv('APY.csv')\n",
    "\n",
    "encoded_df.columns = encoded_df.columns.str.strip()\n",
    "\n",
    "encoded_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset the index\n",
    "encoded_df = encoded_df.reset_index()\n",
    "encoded_df = encoded_df.dropna() \n",
    "# encoded_df['question'] = encoded_df['question'].astype(str).fillna('')\n",
    "encoded_df['question'] = 'What is the yield for ' + encoded_df['Crop'] + ' in ' + encoded_df['Crop_Year'].astype(str) + ' during ' + encoded_df['Season'] + '?'\n",
    "encoded_df['question'].head()\n",
    "\n",
    "split_point = int(round(encoded_df.shape[0] * 0.7, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_df = encoded_df.iloc[:split_point]\n",
    "eval_df = encoded_df.iloc[split_point::]\n",
    "\n",
    "training = training_df[['question']]\n",
    "evaluation = eval_df[['question']]\n",
    "\n",
    "training.tail()\n",
    "\n",
    "from datasets import Dataset\n",
    "# Convert the tokenized data to a Hugging Face Dataset\n",
    "training_set = Dataset.from_pandas(training)\n",
    "eval_set = Dataset.from_pandas(evaluation)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('tiiuae/falcon-7b')\n",
    "\n",
    "# Tokenize the 'question' column\n",
    "tokenized_training = tokenizer(encoded_df['question'].tolist(), truncation=True, max_length=512)\n",
    "tokenized_eval = tokenizer(encoded_df['question'].tolist(), truncation=True, max_length=512)\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert the tokenized data to a Hugging Face Dataset\n",
    "training_dataset = Dataset.from_dict(tokenized_training)\n",
    "eval_dataset = Dataset.from_dict(tokenized_eval)\n",
    "\n",
    "# Save the tokenized dataset to disk\n",
    "tokenized_training.save_to_disk(\"./training_dataset\")\n",
    "tokenized_eval.save_to_disk(\"./eval_dataset\")\n",
    "\n",
    "\n",
    "# Save the tokenized dataset to disk\n",
    "training_dataset.save_to_disk(\"./training_dataset\")\n",
    "eval_dataset.save_to_disk(\"./eval_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
